{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how a Variational Auto-Encoder (VAE) works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + std * eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        h_e = self.encoder(x)\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "\n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if (mu_e is None) and (log_var_e is None):\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None):\n",
    "                raise ValueError('mu and log-var can`t be None!')\n",
    "        z = self.reparameterization(mu_e, log_var_e)\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
    "                raise ValueError('mu, log-var and z can`t be None!')\n",
    "\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = decoder_net\n",
    "        self.distribution = distribution\n",
    "        self.num_vals=num_vals\n",
    "\n",
    "    def decode(self, z):\n",
    "        h_d = self.decoder(z)\n",
    "\n",
    "        if self.distribution == 'categorical':\n",
    "            b = h_d.shape[0]\n",
    "            d = h_d.shape[1]//self.num_vals\n",
    "            h_d = h_d.view(b, d, self.num_vals)\n",
    "            mu_d = torch.softmax(h_d, 2)\n",
    "            return [mu_d]\n",
    "\n",
    "        elif self.distribution == 'bernoulli':\n",
    "            mu_d = torch.sigmoid(h_d)\n",
    "            return [mu_d]\n",
    "        \n",
    "        # Try to take out log variance - Is it correct way?\n",
    "        elif self.distribution == 'normal':\n",
    "            mu_d, log_var_d = torch.chunk(h_d, 2, dim=1)\n",
    "            return [mu_d, log_var_d ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Either `categorical` or `bernoulli` or `normal`')\n",
    "\n",
    "    def sample(self, z):\n",
    "        outs = self.decode(z)\n",
    "\n",
    "        if self.distribution == 'categorical':\n",
    "            mu_d = outs[0]\n",
    "            b = mu_d.shape[0]\n",
    "            m = mu_d.shape[1]\n",
    "            mu_d = mu_d.view(mu_d.shape[0], -1, self.num_vals)\n",
    "            p = mu_d.view(-1, self.num_vals)\n",
    "            x_new = torch.multinomial(p, num_samples=1).view(b, m)\n",
    "\n",
    "        elif self.distribution == 'bernoulli':\n",
    "            mu_d = outs[0]\n",
    "            x_new = torch.bernoulli(mu_d)\n",
    "            \n",
    "        elif self.distribution == 'normal': # changed here\n",
    "            #TODO double check if sampling works as intended\n",
    "            # Torch.normal take standard deviation so performed \n",
    "            # First, antilog then square root og log_var_d and get the std_d\n",
    "            mu_d = outs[0]\n",
    "            std_d = torch.sqrt(torch.exp(outs[1]))\n",
    "            x_new = torch.normal(mu_d, std_d)\n",
    "            x_new = mu_d\n",
    "         \n",
    "        else:\n",
    "            raise ValueError('Either `categorical` or `bernoulli` or `normal`')\n",
    "\n",
    "        return x_new\n",
    "\n",
    "    def log_prob(self, x, z):\n",
    "        outs = self.decode(z)\n",
    "\n",
    "        if self.distribution == 'categorical':\n",
    "            mu_d = outs[0]\n",
    "            log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum', dim=-1).sum(-1)\n",
    "            \n",
    "        elif self.distribution == 'bernoulli':\n",
    "            mu_d = outs[0]\n",
    "            log_p = log_bernoulli(x, mu_d, reduction='sum', dim=-1)\n",
    "        \n",
    "        elif self.distribution == 'normal':# changed here\n",
    "            mu_d = outs[0]\n",
    "            # TODO: estimate in the decoder part\n",
    "            log_var_d = torch.zeros_like(mu_d) # log variance should be 0 if we assume the standard normal variance of 1 \n",
    "            #log_var_d = outs[1]\n",
    "            log_p = log_normal_diag(x, mu_d, log_var_d, reduction='sum', dim=-1)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Either `categorical` or `bernoulli` or `normal`')\n",
    "\n",
    "        return log_p\n",
    "\n",
    "    def forward(self, z, x=None, type='log_prob'):\n",
    "        assert type in ['decoder', 'log_prob'], 'Type could be either decode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x, z)\n",
    "        else:\n",
    "            return self.sample(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # ELBO\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return -(RE + KL).sum()\n",
    "        else:\n",
    "            return -(RE + KL).mean()\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def samples_reconstructed(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 1\n",
    "    num_y = 1\n",
    "    x_real = next(iter(test_loader)).detach().numpy()\n",
    "    mu_e, log_var_e = model_best.encoder.encode(x_real)\n",
    "    z = model_best.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "    x = model_best.decoder.decode(z)[0]\n",
    "    \n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 10000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + torch.rand(batch.shape)\n",
    "            \n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "                \n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "                \n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'vae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "L = 16  # number of latents\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 500 # max. number of epochs\n",
    "max_patience = 50 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE by JT.\n",
      "ENCODER:\n",
      " -----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 256]          16,640          16,640\n",
      "       LeakyReLU-2            [1, 256]               0               0\n",
      "          Linear-3            [1, 256]          65,792          65,792\n",
      "       LeakyReLU-4            [1, 256]               0               0\n",
      "          Linear-5             [1, 32]           8,224           8,224\n",
      "=======================================================================\n",
      "Total params: 90,656\n",
      "Trainable params: 90,656\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "DECODER:\n",
      " -----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 256]           4,352           4,352\n",
      "       LeakyReLU-2            [1, 256]               0               0\n",
      "          Linear-3            [1, 256]          65,792          65,792\n",
      "       LeakyReLU-4            [1, 256]               0               0\n",
      "          Linear-5            [1, 128]          32,896          32,896\n",
      "=======================================================================\n",
      "Total params: 103,040\n",
      "Trainable params: 103,040\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "likelihood_type = 'normal'\n",
    "\n",
    "if likelihood_type == 'categorical':\n",
    "    num_vals = 17\n",
    "elif likelihood_type == 'bernoulli':\n",
    "    num_vals = 1\n",
    "elif likelihood_type == 'normal': # TODO return log variance estimates as well\n",
    "    num_vals = 2\n",
    "\n",
    "encoder = nn.Sequential(nn.Linear(D, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, 2 * L))\n",
    "\n",
    "decoder = nn.Sequential(nn.Linear(L, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                        nn.Linear(M, num_vals * D))\n",
    "\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(L), torch.eye(L))\n",
    "model = VAE(encoder_net=encoder, decoder_net=decoder, num_vals=num_vals, L=L, likelihood_type=likelihood_type)\n",
    "\n",
    "# Print the summary (like in Keras)\n",
    "print(\"ENCODER:\\n\", summary(encoder, torch.zeros(1, D), show_input=False, show_hierarchical=False))\n",
    "print(\"\\nDECODER:\\n\", summary(decoder, torch.zeros(1, L), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=4576.174910714286\n",
      "saved!\n",
      "Epoch: 1, val nll=4427.0215625\n",
      "saved!\n",
      "Epoch: 2, val nll=4402.899553571428\n",
      "saved!\n",
      "Epoch: 3, val nll=4393.146316964286\n",
      "saved!\n",
      "Epoch: 4, val nll=4377.622834821429\n",
      "saved!\n",
      "Epoch: 5, val nll=4346.770401785714\n",
      "saved!\n",
      "Epoch: 6, val nll=4315.322276785714\n",
      "saved!\n",
      "Epoch: 7, val nll=4284.584642857143\n",
      "saved!\n",
      "Epoch: 8, val nll=4248.347366071429\n",
      "saved!\n",
      "Epoch: 9, val nll=4207.661339285714\n",
      "saved!\n",
      "Epoch: 10, val nll=4172.156852678571\n",
      "saved!\n",
      "Epoch: 11, val nll=4149.117723214285\n",
      "saved!\n",
      "Epoch: 12, val nll=4133.801205357143\n",
      "saved!\n",
      "Epoch: 13, val nll=4124.355200892857\n",
      "saved!\n",
      "Epoch: 14, val nll=4118.461383928571\n",
      "saved!\n",
      "Epoch: 15, val nll=4107.314397321428\n",
      "saved!\n",
      "Epoch: 16, val nll=4099.086941964286\n",
      "saved!\n",
      "Epoch: 17, val nll=4088.684419642857\n",
      "saved!\n",
      "Epoch: 18, val nll=4077.2517410714286\n",
      "saved!\n",
      "Epoch: 19, val nll=4065.671875\n",
      "saved!\n",
      "Epoch: 20, val nll=4057.1863839285716\n",
      "saved!\n",
      "Epoch: 21, val nll=4051.2731919642856\n",
      "saved!\n",
      "Epoch: 22, val nll=4042.3127901785715\n",
      "saved!\n",
      "Epoch: 23, val nll=4037.7005580357145\n",
      "saved!\n",
      "Epoch: 24, val nll=4031.0071875\n",
      "saved!\n",
      "Epoch: 25, val nll=4025.7468526785715\n",
      "saved!\n",
      "Epoch: 26, val nll=4022.9396205357143\n",
      "saved!\n",
      "Epoch: 27, val nll=4017.5505580357144\n",
      "saved!\n",
      "Epoch: 28, val nll=4013.944419642857\n",
      "saved!\n",
      "Epoch: 29, val nll=4009.131919642857\n",
      "saved!\n",
      "Epoch: 30, val nll=4003.6935044642855\n",
      "saved!\n",
      "Epoch: 31, val nll=3998.6208035714285\n",
      "saved!\n",
      "Epoch: 32, val nll=3993.749955357143\n",
      "saved!\n",
      "Epoch: 33, val nll=3990.9255357142856\n",
      "saved!\n",
      "Epoch: 34, val nll=3988.0815401785712\n",
      "saved!\n",
      "Epoch: 35, val nll=3982.9680133928573\n",
      "saved!\n",
      "Epoch: 36, val nll=3982.2565625\n",
      "saved!\n",
      "Epoch: 37, val nll=3979.694285714286\n",
      "saved!\n",
      "Epoch: 38, val nll=3976.6722098214286\n",
      "saved!\n",
      "Epoch: 39, val nll=3975.474732142857\n",
      "saved!\n",
      "Epoch: 40, val nll=3971.0301339285716\n",
      "saved!\n",
      "Epoch: 41, val nll=3969.4265848214286\n",
      "saved!\n",
      "Epoch: 42, val nll=3966.8226785714287\n",
      "saved!\n",
      "Epoch: 43, val nll=3965.733348214286\n",
      "saved!\n",
      "Epoch: 44, val nll=3964.7855357142857\n",
      "saved!\n",
      "Epoch: 45, val nll=3961.172455357143\n",
      "saved!\n",
      "Epoch: 46, val nll=3961.923705357143\n",
      "Epoch: 47, val nll=3958.5854464285712\n",
      "saved!\n",
      "Epoch: 48, val nll=3955.6383482142855\n",
      "saved!\n",
      "Epoch: 49, val nll=3955.52328125\n",
      "saved!\n",
      "Epoch: 50, val nll=3951.7651339285712\n",
      "saved!\n",
      "Epoch: 51, val nll=3950.7702232142856\n",
      "saved!\n",
      "Epoch: 52, val nll=3948.2156919642857\n",
      "saved!\n",
      "Epoch: 53, val nll=3945.508348214286\n",
      "saved!\n",
      "Epoch: 54, val nll=3943.9596651785714\n",
      "saved!\n",
      "Epoch: 55, val nll=3942.7814285714285\n",
      "saved!\n",
      "Epoch: 56, val nll=3941.689464285714\n",
      "saved!\n",
      "Epoch: 57, val nll=3938.613214285714\n",
      "saved!\n",
      "Epoch: 58, val nll=3937.309174107143\n",
      "saved!\n",
      "Epoch: 59, val nll=3935.4379910714288\n",
      "saved!\n",
      "Epoch: 60, val nll=3934.026339285714\n",
      "saved!\n",
      "Epoch: 61, val nll=3932.2340625\n",
      "saved!\n",
      "Epoch: 62, val nll=3930.7882589285714\n",
      "saved!\n",
      "Epoch: 63, val nll=3929.2131473214286\n",
      "saved!\n",
      "Epoch: 64, val nll=3929.000892857143\n",
      "saved!\n",
      "Epoch: 65, val nll=3927.128504464286\n",
      "saved!\n",
      "Epoch: 66, val nll=3927.3373214285716\n",
      "Epoch: 67, val nll=3924.1569196428572\n",
      "saved!\n",
      "Epoch: 68, val nll=3923.8469642857144\n",
      "saved!\n",
      "Epoch: 69, val nll=3924.097098214286\n",
      "Epoch: 70, val nll=3923.5330133928574\n",
      "saved!\n",
      "Epoch: 71, val nll=3920.1789508928573\n",
      "saved!\n",
      "Epoch: 72, val nll=3919.2381919642858\n",
      "saved!\n",
      "Epoch: 73, val nll=3918.4713839285714\n",
      "saved!\n",
      "Epoch: 74, val nll=3918.905223214286\n",
      "Epoch: 75, val nll=3918.001941964286\n",
      "saved!\n",
      "Epoch: 76, val nll=3916.0883035714287\n",
      "saved!\n",
      "Epoch: 77, val nll=3916.190580357143\n",
      "Epoch: 78, val nll=3915.8088169642856\n",
      "saved!\n",
      "Epoch: 79, val nll=3913.8536607142855\n",
      "saved!\n",
      "Epoch: 80, val nll=3913.5617857142856\n",
      "saved!\n",
      "Epoch: 81, val nll=3913.360357142857\n",
      "saved!\n",
      "Epoch: 82, val nll=3913.7188839285714\n",
      "Epoch: 83, val nll=3910.7363839285713\n",
      "saved!\n",
      "Epoch: 84, val nll=3909.8831473214286\n",
      "saved!\n",
      "Epoch: 85, val nll=3910.770044642857\n",
      "Epoch: 86, val nll=3909.480044642857\n",
      "saved!\n",
      "Epoch: 87, val nll=3909.0276116071427\n",
      "saved!\n",
      "Epoch: 88, val nll=3909.0569196428573\n",
      "Epoch: 89, val nll=3908.4149107142857\n",
      "saved!\n",
      "Epoch: 90, val nll=3907.1077901785716\n",
      "saved!\n",
      "Epoch: 91, val nll=3906.8352901785715\n",
      "saved!\n",
      "Epoch: 92, val nll=3906.7258705357144\n",
      "saved!\n",
      "Epoch: 93, val nll=3906.095044642857\n",
      "saved!\n",
      "Epoch: 94, val nll=3905.3907589285714\n",
      "saved!\n",
      "Epoch: 95, val nll=3905.12375\n",
      "saved!\n",
      "Epoch: 96, val nll=3904.9206473214285\n",
      "saved!\n",
      "Epoch: 97, val nll=3905.0491071428573\n",
      "Epoch: 98, val nll=3905.012700892857\n",
      "Epoch: 99, val nll=3902.1969642857143\n",
      "saved!\n",
      "Epoch: 100, val nll=3903.0264955357143\n",
      "Epoch: 101, val nll=3903.1204241071428\n",
      "Epoch: 102, val nll=3902.096205357143\n",
      "saved!\n",
      "Epoch: 103, val nll=3900.65203125\n",
      "saved!\n",
      "Epoch: 104, val nll=3900.8882589285713\n",
      "Epoch: 105, val nll=3900.0945982142857\n",
      "saved!\n",
      "Epoch: 106, val nll=3899.5363392857143\n",
      "saved!\n",
      "Epoch: 107, val nll=3900.5452232142857\n",
      "Epoch: 108, val nll=3899.4168303571428\n",
      "saved!\n",
      "Epoch: 109, val nll=3899.7683258928573\n",
      "Epoch: 110, val nll=3898.9640848214285\n",
      "saved!\n",
      "Epoch: 111, val nll=3899.0686607142857\n",
      "Epoch: 112, val nll=3899.597299107143\n",
      "Epoch: 113, val nll=3897.6686160714285\n",
      "saved!\n",
      "Epoch: 114, val nll=3897.769642857143\n",
      "Epoch: 115, val nll=3898.9859375\n",
      "Epoch: 116, val nll=3897.8303794642857\n",
      "Epoch: 117, val nll=3896.7866964285713\n",
      "saved!\n",
      "Epoch: 118, val nll=3896.120982142857\n",
      "saved!\n",
      "Epoch: 119, val nll=3896.865223214286\n",
      "Epoch: 120, val nll=3894.4553571428573\n",
      "saved!\n",
      "Epoch: 121, val nll=3895.4745089285716\n",
      "Epoch: 122, val nll=3894.766183035714\n",
      "Epoch: 123, val nll=3894.914620535714\n",
      "Epoch: 124, val nll=3895.6985714285715\n",
      "Epoch: 125, val nll=3893.883816964286\n",
      "saved!\n",
      "Epoch: 126, val nll=3892.9832366071428\n",
      "saved!\n",
      "Epoch: 127, val nll=3894.1159821428573\n",
      "Epoch: 128, val nll=3893.29421875\n",
      "Epoch: 129, val nll=3892.7614955357144\n",
      "saved!\n",
      "Epoch: 130, val nll=3892.6701339285714\n",
      "saved!\n",
      "Epoch: 131, val nll=3893.7691517857143\n",
      "Epoch: 132, val nll=3892.323861607143\n",
      "saved!\n",
      "Epoch: 133, val nll=3892.5910491071427\n",
      "Epoch: 134, val nll=3891.61703125\n",
      "saved!\n",
      "Epoch: 135, val nll=3891.001607142857\n",
      "saved!\n",
      "Epoch: 136, val nll=3891.18546875\n",
      "Epoch: 137, val nll=3890.4499776785715\n",
      "saved!\n",
      "Epoch: 138, val nll=3892.5015401785713\n",
      "Epoch: 139, val nll=3890.42671875\n",
      "saved!\n",
      "Epoch: 140, val nll=3890.139799107143\n",
      "saved!\n",
      "Epoch: 141, val nll=3889.873705357143\n",
      "saved!\n",
      "Epoch: 142, val nll=3890.000982142857\n",
      "Epoch: 143, val nll=3889.5372098214284\n",
      "saved!\n",
      "Epoch: 144, val nll=3889.5802901785714\n",
      "Epoch: 145, val nll=3889.2970089285714\n",
      "saved!\n",
      "Epoch: 146, val nll=3888.566473214286\n",
      "saved!\n",
      "Epoch: 147, val nll=3888.2289285714287\n",
      "saved!\n",
      "Epoch: 148, val nll=3887.5965178571428\n",
      "saved!\n",
      "Epoch: 149, val nll=3887.9506473214287\n",
      "Epoch: 150, val nll=3887.240714285714\n",
      "saved!\n",
      "Epoch: 151, val nll=3887.504910714286\n",
      "Epoch: 152, val nll=3888.006919642857\n",
      "Epoch: 153, val nll=3887.45625\n",
      "Epoch: 154, val nll=3885.9929910714286\n",
      "saved!\n",
      "Epoch: 155, val nll=3888.580200892857\n",
      "Epoch: 156, val nll=3887.1598660714285\n",
      "Epoch: 157, val nll=3886.8959375\n",
      "Epoch: 158, val nll=3886.8842410714287\n",
      "Epoch: 159, val nll=3886.409799107143\n",
      "Epoch: 160, val nll=3886.4865401785714\n",
      "Epoch: 161, val nll=3886.1707142857144\n",
      "Epoch: 162, val nll=3887.504799107143\n",
      "Epoch: 163, val nll=3886.47328125\n",
      "Epoch: 164, val nll=3885.3610491071427\n",
      "saved!\n",
      "Epoch: 165, val nll=3885.2664955357145\n",
      "saved!\n",
      "Epoch: 166, val nll=3885.226919642857\n",
      "saved!\n",
      "Epoch: 167, val nll=3885.096875\n",
      "saved!\n",
      "Epoch: 168, val nll=3886.373013392857\n",
      "Epoch: 169, val nll=3885.6222321428572\n",
      "Epoch: 170, val nll=3887.001830357143\n",
      "Epoch: 171, val nll=3885.4562946428573\n",
      "Epoch: 172, val nll=3885.5837946428574\n",
      "Epoch: 173, val nll=3885.0017410714286\n",
      "saved!\n",
      "Epoch: 174, val nll=3885.6730580357143\n",
      "Epoch: 175, val nll=3884.672276785714\n",
      "saved!\n",
      "Epoch: 176, val nll=3884.5910267857143\n",
      "saved!\n",
      "Epoch: 177, val nll=3883.3762276785715\n",
      "saved!\n",
      "Epoch: 178, val nll=3885.089776785714\n",
      "Epoch: 179, val nll=3884.9861830357145\n",
      "Epoch: 180, val nll=3884.1887723214286\n",
      "Epoch: 181, val nll=3884.484017857143\n",
      "Epoch: 182, val nll=3885.25109375\n",
      "Epoch: 183, val nll=3882.92625\n",
      "saved!\n",
      "Epoch: 184, val nll=3882.27\n",
      "saved!\n",
      "Epoch: 185, val nll=3883.3271875\n",
      "Epoch: 186, val nll=3883.0147544642855\n",
      "Epoch: 187, val nll=3883.433950892857\n",
      "Epoch: 188, val nll=3882.4242410714287\n",
      "Epoch: 189, val nll=3884.2228571428573\n",
      "Epoch: 190, val nll=3882.9398214285716\n",
      "Epoch: 191, val nll=3882.9331026785712\n",
      "Epoch: 192, val nll=3882.28421875\n",
      "Epoch: 193, val nll=3882.798325892857\n",
      "Epoch: 194, val nll=3882.598482142857\n",
      "Epoch: 195, val nll=3882.1243080357144\n",
      "saved!\n",
      "Epoch: 196, val nll=3882.809107142857\n",
      "Epoch: 197, val nll=3882.2770535714285\n",
      "Epoch: 198, val nll=3881.528839285714\n",
      "saved!\n",
      "Epoch: 199, val nll=3880.922276785714\n",
      "saved!\n",
      "Epoch: 200, val nll=3880.9899776785714\n",
      "Epoch: 201, val nll=3882.181517857143\n",
      "Epoch: 202, val nll=3881.695200892857\n",
      "Epoch: 203, val nll=3881.6921651785715\n",
      "Epoch: 204, val nll=3882.271049107143\n",
      "Epoch: 205, val nll=3881.4615848214285\n",
      "Epoch: 206, val nll=3881.269598214286\n",
      "Epoch: 207, val nll=3881.1357589285712\n",
      "Epoch: 208, val nll=3881.27\n",
      "Epoch: 209, val nll=3882.1853794642857\n",
      "Epoch: 210, val nll=3881.1002455357143\n",
      "Epoch: 211, val nll=3881.6970535714286\n",
      "Epoch: 212, val nll=3880.4142857142856\n",
      "saved!\n",
      "Epoch: 213, val nll=3880.4061607142858\n",
      "saved!\n",
      "Epoch: 214, val nll=3881.2438392857143\n",
      "Epoch: 215, val nll=3880.9989285714287\n",
      "Epoch: 216, val nll=3880.3494642857145\n",
      "saved!\n",
      "Epoch: 217, val nll=3879.875424107143\n",
      "saved!\n",
      "Epoch: 218, val nll=3880.3925\n",
      "Epoch: 219, val nll=3879.7275223214288\n",
      "saved!\n",
      "Epoch: 220, val nll=3880.8865178571427\n",
      "Epoch: 221, val nll=3881.069017857143\n",
      "Epoch: 222, val nll=3880.27296875\n",
      "Epoch: 223, val nll=3880.1638169642856\n",
      "Epoch: 224, val nll=3879.0504910714285\n",
      "saved!\n",
      "Epoch: 225, val nll=3879.2126785714286\n",
      "Epoch: 226, val nll=3879.4544419642857\n",
      "Epoch: 227, val nll=3880.424330357143\n",
      "Epoch: 228, val nll=3880.430669642857\n",
      "Epoch: 229, val nll=3879.245580357143\n",
      "Epoch: 230, val nll=3879.8548660714287\n",
      "Epoch: 231, val nll=3880.6942410714287\n",
      "Epoch: 232, val nll=3879.493660714286\n",
      "Epoch: 233, val nll=3878.94703125\n",
      "saved!\n",
      "Epoch: 234, val nll=3880.182299107143\n",
      "Epoch: 235, val nll=3880.8018526785713\n",
      "Epoch: 236, val nll=3879.1618526785714\n",
      "Epoch: 237, val nll=3879.6798214285714\n",
      "Epoch: 238, val nll=3878.85\n",
      "saved!\n",
      "Epoch: 239, val nll=3879.6514955357143\n",
      "Epoch: 240, val nll=3879.1154464285714\n",
      "Epoch: 241, val nll=3878.87640625\n",
      "Epoch: 242, val nll=3878.9612053571427\n",
      "Epoch: 243, val nll=3878.8155357142855\n",
      "saved!\n",
      "Epoch: 244, val nll=3878.807232142857\n",
      "saved!\n",
      "Epoch: 245, val nll=3878.0707589285716\n",
      "saved!\n",
      "Epoch: 246, val nll=3878.621607142857\n",
      "Epoch: 247, val nll=3878.3707589285714\n",
      "Epoch: 248, val nll=3878.2268303571427\n",
      "Epoch: 249, val nll=3878.2361607142857\n",
      "Epoch: 250, val nll=3878.7508035714286\n",
      "Epoch: 251, val nll=3878.7649776785715\n",
      "Epoch: 252, val nll=3877.97703125\n",
      "saved!\n",
      "Epoch: 253, val nll=3878.9574107142857\n",
      "Epoch: 254, val nll=3878.8927678571426\n",
      "Epoch: 255, val nll=3878.604107142857\n",
      "Epoch: 256, val nll=3878.9461830357145\n",
      "Epoch: 257, val nll=3877.950669642857\n",
      "saved!\n",
      "Epoch: 258, val nll=3877.1459151785716\n",
      "saved!\n",
      "Epoch: 259, val nll=3877.7697321428573\n",
      "Epoch: 260, val nll=3877.942299107143\n",
      "Epoch: 261, val nll=3878.4149776785716\n",
      "Epoch: 262, val nll=3878.4729017857144\n",
      "Epoch: 263, val nll=3878.1165401785715\n",
      "Epoch: 264, val nll=3877.0118080357142\n",
      "saved!\n",
      "Epoch: 265, val nll=3877.5207589285715\n",
      "Epoch: 266, val nll=3878.89859375\n",
      "Epoch: 267, val nll=3877.199151785714\n",
      "Epoch: 268, val nll=3878.4245758928573\n",
      "Epoch: 269, val nll=3878.1381473214287\n",
      "Epoch: 270, val nll=3879.398482142857\n",
      "Epoch: 271, val nll=3877.15859375\n",
      "Epoch: 272, val nll=3877.8566071428572\n",
      "Epoch: 273, val nll=3877.0695089285714\n",
      "Epoch: 274, val nll=3877.564955357143\n",
      "Epoch: 275, val nll=3877.383705357143\n",
      "Epoch: 276, val nll=3877.6027901785715\n",
      "Epoch: 277, val nll=3877.3612723214287\n",
      "Epoch: 278, val nll=3877.5124107142856\n",
      "Epoch: 279, val nll=3878.4087723214284\n",
      "Epoch: 280, val nll=3877.174799107143\n",
      "Epoch: 281, val nll=3878.4046205357145\n",
      "Epoch: 282, val nll=3877.7658258928573\n",
      "Epoch: 283, val nll=3876.6999107142856\n",
      "saved!\n",
      "Epoch: 284, val nll=3878.15265625\n",
      "Epoch: 285, val nll=3876.9283705357143\n",
      "Epoch: 286, val nll=3878.15578125\n",
      "Epoch: 287, val nll=3877.34796875\n",
      "Epoch: 288, val nll=3876.9054464285714\n",
      "Epoch: 289, val nll=3877.5598214285715\n",
      "Epoch: 290, val nll=3876.4959598214286\n",
      "saved!\n",
      "Epoch: 291, val nll=3876.579017857143\n",
      "Epoch: 292, val nll=3877.8155357142855\n",
      "Epoch: 293, val nll=3876.4588169642857\n",
      "saved!\n",
      "Epoch: 294, val nll=3877.196316964286\n",
      "Epoch: 295, val nll=3876.179330357143\n",
      "saved!\n",
      "Epoch: 296, val nll=3877.933236607143\n",
      "Epoch: 297, val nll=3877.4689955357144\n",
      "Epoch: 298, val nll=3877.02609375\n",
      "Epoch: 299, val nll=3877.0176339285713\n",
      "Epoch: 300, val nll=3878.3248214285713\n",
      "Epoch: 301, val nll=3876.63546875\n",
      "Epoch: 302, val nll=3876.2541517857144\n",
      "Epoch: 303, val nll=3877.1362276785712\n",
      "Epoch: 304, val nll=3877.1663839285716\n",
      "Epoch: 305, val nll=3876.8646651785716\n",
      "Epoch: 306, val nll=3876.6037723214286\n",
      "Epoch: 307, val nll=3876.678392857143\n",
      "Epoch: 308, val nll=3877.3582142857144\n",
      "Epoch: 309, val nll=3877.4674776785714\n",
      "Epoch: 310, val nll=3876.5812276785714\n",
      "Epoch: 311, val nll=3876.549419642857\n",
      "Epoch: 312, val nll=3876.76328125\n",
      "Epoch: 313, val nll=3875.640044642857\n",
      "saved!\n",
      "Epoch: 314, val nll=3877.345044642857\n",
      "Epoch: 315, val nll=3877.274955357143\n",
      "Epoch: 316, val nll=3876.648013392857\n",
      "Epoch: 317, val nll=3875.2303794642858\n",
      "saved!\n",
      "Epoch: 318, val nll=3875.8908035714285\n",
      "Epoch: 319, val nll=3875.8536160714284\n",
      "Epoch: 320, val nll=3875.546763392857\n",
      "Epoch: 321, val nll=3876.3688839285714\n",
      "Epoch: 322, val nll=3876.599017857143\n",
      "Epoch: 323, val nll=3875.8469642857144\n",
      "Epoch: 324, val nll=3875.8366071428572\n",
      "Epoch: 325, val nll=3875.3704910714287\n",
      "Epoch: 326, val nll=3876.785111607143\n",
      "Epoch: 327, val nll=3876.4396875\n",
      "Epoch: 328, val nll=3876.7253125\n",
      "Epoch: 329, val nll=3877.253549107143\n",
      "Epoch: 330, val nll=3876.5558035714284\n",
      "Epoch: 331, val nll=3876.0075669642856\n",
      "Epoch: 332, val nll=3877.691517857143\n",
      "Epoch: 333, val nll=3876.6234375\n",
      "Epoch: 334, val nll=3876.0645535714284\n",
      "Epoch: 335, val nll=3876.313794642857\n",
      "Epoch: 336, val nll=3876.337857142857\n",
      "Epoch: 337, val nll=3875.521495535714\n",
      "Epoch: 338, val nll=3875.934598214286\n",
      "Epoch: 339, val nll=3877.4202455357145\n",
      "Epoch: 340, val nll=3875.0280580357144\n",
      "saved!\n",
      "Epoch: 341, val nll=3875.6642410714285\n",
      "Epoch: 342, val nll=3875.735736607143\n",
      "Epoch: 343, val nll=3875.7434375\n",
      "Epoch: 344, val nll=3876.8663839285714\n",
      "Epoch: 345, val nll=3876.578660714286\n",
      "Epoch: 346, val nll=3875.7293303571428\n",
      "Epoch: 347, val nll=3875.902767857143\n",
      "Epoch: 348, val nll=3874.73296875\n",
      "saved!\n",
      "Epoch: 349, val nll=3876.6183928571427\n",
      "Epoch: 350, val nll=3875.4404910714284\n",
      "Epoch: 351, val nll=3875.4033035714288\n",
      "Epoch: 352, val nll=3875.5269642857143\n",
      "Epoch: 353, val nll=3875.2521651785714\n",
      "Epoch: 354, val nll=3875.7991517857145\n",
      "Epoch: 355, val nll=3874.895424107143\n",
      "Epoch: 356, val nll=3875.5262723214287\n",
      "Epoch: 357, val nll=3876.3170982142856\n",
      "Epoch: 358, val nll=3876.4383705357145\n",
      "Epoch: 359, val nll=3875.6103348214288\n",
      "Epoch: 360, val nll=3875.021361607143\n",
      "Epoch: 361, val nll=3875.3855803571428\n",
      "Epoch: 362, val nll=3875.8384375\n",
      "Epoch: 363, val nll=3876.893950892857\n",
      "Epoch: 364, val nll=3874.958035714286\n",
      "Epoch: 365, val nll=3875.129799107143\n",
      "Epoch: 366, val nll=3875.9596875\n",
      "Epoch: 367, val nll=3875.251964285714\n",
      "Epoch: 368, val nll=3875.1671875\n",
      "Epoch: 369, val nll=3876.1849776785716\n",
      "Epoch: 370, val nll=3876.0101785714287\n",
      "Epoch: 371, val nll=3875.582566964286\n",
      "Epoch: 372, val nll=3875.634776785714\n",
      "Epoch: 373, val nll=3876.518236607143\n",
      "Epoch: 374, val nll=3875.6436160714284\n",
      "Epoch: 375, val nll=3875.5268973214284\n",
      "Epoch: 376, val nll=3876.2957366071428\n",
      "Epoch: 377, val nll=3875.6508035714287\n",
      "Epoch: 378, val nll=3875.759642857143\n",
      "Epoch: 379, val nll=3876.83546875\n",
      "Epoch: 380, val nll=3875.5992410714284\n",
      "Epoch: 381, val nll=3875.001495535714\n",
      "Epoch: 382, val nll=3875.720825892857\n",
      "Epoch: 383, val nll=3875.339888392857\n",
      "Epoch: 384, val nll=3874.7936830357144\n",
      "Epoch: 385, val nll=3875.801205357143\n",
      "Epoch: 386, val nll=3875.1638392857144\n",
      "Epoch: 387, val nll=3875.20234375\n",
      "Epoch: 388, val nll=3876.2740401785713\n",
      "Epoch: 389, val nll=3875.9295535714286\n",
      "Epoch: 390, val nll=3876.9011383928573\n",
      "Epoch: 391, val nll=3874.7949776785713\n",
      "Epoch: 392, val nll=3876.1408258928573\n",
      "Epoch: 393, val nll=3875.8708035714285\n",
      "Epoch: 394, val nll=3875.4602008928573\n",
      "Epoch: 395, val nll=3876.0695089285714\n",
      "Epoch: 396, val nll=3874.877857142857\n",
      "Epoch: 397, val nll=3875.5984598214286\n",
      "Epoch: 398, val nll=3874.461875\n",
      "saved!\n",
      "Epoch: 399, val nll=3875.8938839285715\n",
      "Epoch: 400, val nll=3874.9629464285713\n",
      "Epoch: 401, val nll=3875.5438392857145\n",
      "Epoch: 402, val nll=3875.601629464286\n",
      "Epoch: 403, val nll=3875.532276785714\n",
      "Epoch: 404, val nll=3875.7404241071426\n",
      "Epoch: 405, val nll=3874.6051785714285\n",
      "Epoch: 406, val nll=3875.3810267857143\n",
      "Epoch: 407, val nll=3874.9225\n",
      "Epoch: 408, val nll=3875.8908928571427\n",
      "Epoch: 409, val nll=3875.1223214285715\n",
      "Epoch: 410, val nll=3876.0550669642857\n",
      "Epoch: 411, val nll=3875.9507589285713\n",
      "Epoch: 412, val nll=3875.2017857142855\n",
      "Epoch: 413, val nll=3875.3860491071428\n",
      "Epoch: 414, val nll=3874.88375\n",
      "Epoch: 415, val nll=3875.8579464285713\n",
      "Epoch: 416, val nll=3875.3824776785714\n",
      "Epoch: 417, val nll=3875.339285714286\n",
      "Epoch: 418, val nll=3876.0551339285716\n",
      "Epoch: 419, val nll=3875.3620089285714\n",
      "Epoch: 420, val nll=3875.129308035714\n",
      "Epoch: 421, val nll=3876.059486607143\n",
      "Epoch: 422, val nll=3875.142589285714\n",
      "Epoch: 423, val nll=3875.20921875\n",
      "Epoch: 424, val nll=3874.0716964285716\n",
      "saved!\n",
      "Epoch: 425, val nll=3874.78796875\n",
      "Epoch: 426, val nll=3875.061517857143\n",
      "Epoch: 427, val nll=3875.5387946428573\n",
      "Epoch: 428, val nll=3874.308549107143\n",
      "Epoch: 429, val nll=3874.685513392857\n",
      "Epoch: 430, val nll=3874.9079241071427\n",
      "Epoch: 431, val nll=3875.6559598214285\n",
      "Epoch: 432, val nll=3874.9263839285713\n",
      "Epoch: 433, val nll=3874.94\n",
      "Epoch: 434, val nll=3875.1959375\n",
      "Epoch: 435, val nll=3875.4348214285715\n",
      "Epoch: 436, val nll=3874.4372767857144\n",
      "Epoch: 437, val nll=3875.5558482142856\n",
      "Epoch: 438, val nll=3874.071674107143\n",
      "saved!\n",
      "Epoch: 439, val nll=3875.286875\n",
      "Epoch: 440, val nll=3875.538638392857\n",
      "Epoch: 441, val nll=3875.236875\n",
      "Epoch: 442, val nll=3875.892098214286\n",
      "Epoch: 443, val nll=3875.52359375\n",
      "Epoch: 444, val nll=3874.953705357143\n",
      "Epoch: 445, val nll=3874.818950892857\n",
      "Epoch: 446, val nll=3875.5942410714288\n",
      "Epoch: 447, val nll=3875.297232142857\n",
      "Epoch: 448, val nll=3875.8924107142857\n",
      "Epoch: 449, val nll=3875.65046875\n",
      "Epoch: 450, val nll=3874.0221205357143\n",
      "saved!\n",
      "Epoch: 451, val nll=3875.2161607142857\n",
      "Epoch: 452, val nll=3875.3566294642856\n",
      "Epoch: 453, val nll=3875.08671875\n",
      "Epoch: 454, val nll=3874.97390625\n",
      "Epoch: 455, val nll=3874.6739285714284\n",
      "Epoch: 456, val nll=3874.582544642857\n",
      "Epoch: 457, val nll=3875.2594642857143\n",
      "Epoch: 458, val nll=3874.258794642857\n",
      "Epoch: 459, val nll=3875.0196875\n",
      "Epoch: 460, val nll=3874.8863839285714\n",
      "Epoch: 461, val nll=3874.2608035714284\n",
      "Epoch: 462, val nll=3874.114642857143\n",
      "Epoch: 463, val nll=3875.586964285714\n",
      "Epoch: 464, val nll=3874.969375\n",
      "Epoch: 465, val nll=3875.292433035714\n",
      "Epoch: 466, val nll=3875.695892857143\n",
      "Epoch: 467, val nll=3875.2010044642857\n",
      "Epoch: 468, val nll=3876.873973214286\n",
      "Epoch: 469, val nll=3875.3111160714284\n",
      "Epoch: 470, val nll=3875.147879464286\n",
      "Epoch: 471, val nll=3874.247075892857\n",
      "Epoch: 472, val nll=3874.7728348214287\n",
      "Epoch: 473, val nll=3874.549330357143\n",
      "Epoch: 474, val nll=3874.9377901785715\n",
      "Epoch: 475, val nll=3875.48328125\n",
      "Epoch: 476, val nll=3874.2938169642857\n",
      "Epoch: 477, val nll=3874.881986607143\n",
      "Epoch: 478, val nll=3875.0007589285715\n",
      "Epoch: 479, val nll=3875.4310714285716\n",
      "Epoch: 480, val nll=3874.5602232142855\n",
      "Epoch: 481, val nll=3875.0822991071427\n",
      "Epoch: 482, val nll=3874.9587723214286\n",
      "Epoch: 483, val nll=3875.2170089285714\n",
      "Epoch: 484, val nll=3874.4606026785714\n",
      "Epoch: 485, val nll=3875.4914732142856\n",
      "Epoch: 486, val nll=3875.581004464286\n",
      "Epoch: 487, val nll=3875.0757589285713\n",
      "Epoch: 488, val nll=3874.7590625\n",
      "Epoch: 489, val nll=3874.8492857142855\n",
      "Epoch: 490, val nll=3874.441473214286\n",
      "Epoch: 491, val nll=3874.8147321428573\n",
      "Epoch: 492, val nll=3875.2404241071426\n",
      "Epoch: 493, val nll=3876.238861607143\n",
      "Epoch: 494, val nll=3875.3035491071428\n",
      "Epoch: 495, val nll=3874.2596205357145\n",
      "Epoch: 496, val nll=3875.35453125\n",
      "Epoch: 497, val nll=3875.4354910714287\n",
      "Epoch: 498, val nll=3876.1086383928573\n",
      "Epoch: 499, val nll=3875.87765625\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=3870.6199664429532\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[268], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#samples_real(result_dir + name, test_loader)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43msamples_reconstructed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m plot_curve(result_dir \u001b[38;5;241m+\u001b[39m name, nll_val)\n",
      "Cell \u001b[0;32mIn[261], line 71\u001b[0m, in \u001b[0;36msamples_reconstructed\u001b[0;34m(name, data_loader, extra_name)\u001b[0m\n\u001b[1;32m     69\u001b[0m num_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     70\u001b[0m x_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 71\u001b[0m mu_e, log_var_e \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_best\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m z \u001b[38;5;241m=\u001b[39m model_best\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39msample(mu_e\u001b[38;5;241m=\u001b[39mmu_e, log_var_e\u001b[38;5;241m=\u001b[39mlog_var_e)\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m model_best\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mdecode(z)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[257], line 16\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     h_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     mu_e, log_var_e \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(h_e, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu_e, log_var_e\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dgm/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "#samples_real(result_dir + name, test_loader)\n",
    "samples_reconstructed(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
